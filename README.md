This is based on work in my Hugging Face repository here:
https://huggingface.co/datasets/tfnn/HeadsNet

The Hugging Face repository has the full project including the dataset, this GitHub repository just stands to hold the bare code.

It all started with this rough idea that I had after spending much time looking into Neural Radiance Fields (NeRF) for generative 3D:
https://gist.github.com/mrbid/1eacdd9d9239b2d324a3fa88591ff852

The dataset all this was trained on is a synthetic dataset I generated from StyleGAN2 using [ThisPersonDoesNotExist.com](https://ThisPersonDoesNotExist.com) and then feeding those synthetic 2D images into [TripoSR](https://github.com/VAST-AI-Research/TripoSR) to turn them into 3D heads, the dataset is on Hugging Face here: https://huggingface.co/datasets/tfnn/FaceTo3D

The first attempt was my [PT-NePC](https://gist.github.com/mrbid/1eacdd9d9239b2d324a3fa88591ff852) approach in ["headsnet"](headsnet). HeadsNet was the highest quality attempt. It took a simple two vector input to produce a random full color 3D point cloud of a head. It includes the scraper, a viewer for the scraped models, the dataset generator, training and prediction code.

The second attempt was to simplify the problem down to producing a 32^3 grayscale voxel volume of a head from a 32x32 grayscale input image.
- [facenet1](facenet1) has the dataset generation code and the first attempt at FaceToVoxel. It attempts to train one large FNN/MLP on the problem.
- [facenet2](facenet2) requires the dataset generated by facenet1, and attempt to train the problem on 32^3 individual networks with a single output. This allows better parallelisation such as over multiple machines in a network.

This project deliberatley focuses on MLP's while ignoring VAE's which would be a more traditional use case.

Training was done on a single [HPE ProLiant DL580 Gen9](https://www.hpe.com/psnow/doc/c04601208) with [Intel® Xeon® E7-8880 v4](https://www.intel.com/content/www/us/en/products/sku/93792/intel-xeon-processor-e78880-v4-55m-cache-2-20-ghz/specifications.html). Although I could have done with a few of these to be honest! 32 of them would reduced the training process from a week or multiple weeks to just a few hours or days. Being able to perform faster tests allows one to hone in on a working and quaity model much faster.
