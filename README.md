### This repository holds the most upto-date versions of the scripts.

This is based on work in my Hugging Face repository ["HeadsNet"](https://huggingface.co/datasets/tfnn/HeadsNet).

The Hugging Face repository has the full project including the dataset _(I also go into a little more detail)_, this GitHub repository just stands to hold the bare code.

It all started with this rough idea that I had after spending much time looking into Neural Radiance Fields (NeRF) for generative 3D which can be viewed here: ["PT-NePC"](https://gist.github.com/mrbid/1eacdd9d9239b2d324a3fa88591ff852).

The dataset this was trained on is a synthetic dataset I generated from [StyleGAN2](https://github.com/NVlabs/stylegan2) using [ThisPersonDoesNotExist.com](https://ThisPersonDoesNotExist.com) and then feeding those synthetic 2D images into [TripoSR](https://github.com/VAST-AI-Research/TripoSR) to turn them into 3D heads, the dataset is on Hugging Face here: ["FaceTo3D"](https://huggingface.co/datasets/tfnn/FaceTo3D).

**The first attempt** was my [PT-NePC](https://gist.github.com/mrbid/1eacdd9d9239b2d324a3fa88591ff852) approach in ["headsnet"](headsnet). HeadsNet was the highest quality attempt. It took a simple two vector input to produce a random full color 3D point cloud of a head. It includes the scraper, a viewer for the scraped models, the dataset generator, training and prediction code.

**The second attempt** was to simplify the problem down to producing a 32^3 grayscale voxel volume of a head from a 32x32 grayscale input image.
- [facenet1](facenet1) has the dataset generation code and the first attempt at FaceToVoxel. It attempts to train one large FNN/MLP on the problem.
- [facenet2](facenet2) requires the dataset generated by facenet1, and attempts to train the problem on 32^3 individual networks with a single output _(the grayscale value for a single voxel)_. This allows better parallelisation such as over multiple machines in a network - but also better parallelisation over multiple CPU cores in a single computer system.
- [facenet3](facenet3) the successor model, a simplified version of facenet1.

This project deliberately focuses on [MLP's](https://en.wikipedia.org/wiki/Multilayer_perceptron) while ignoring [VAE's](https://en.wikipedia.org/wiki/Variational_autoencoder) which would be a more traditional use case.

Training was done on a single [HPE ProLiant DL580 Gen9](https://www.hpe.com/psnow/doc/c04601208) with [Intel® Xeon® E7-8880 v4](https://www.intel.com/content/www/us/en/products/sku/93792/intel-xeon-processor-e78880-v4-55m-cache-2-20-ghz/specifications.html). Although I could have done with a few of these to be honest! 32 of them would reduce the training process from a week or multiple weeks to just a few hours or days. Being able to perform faster tests allows one to hone in on a working and quality model much faster.

---

An example of ground truth outputs that facenet is trained on is [facenet_ground_truth.7z](https://github.com/mrbid/FaceTo3D/raw/main/facenet_ground_truth.7z).
